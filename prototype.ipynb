{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4f38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba39aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, TypeAdapter\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ab3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://llama.lan:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602f46fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ollama is running'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(OLLAMA_URL).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d6b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(\n",
    "    model_name='llama3.2',\n",
    "    provider=OpenAIProvider(base_url=f'{OLLAMA_URL}/v1')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fba0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaModelDetails(BaseModel):\n",
    "    parent_model: str\n",
    "    format: str\n",
    "    family: str\n",
    "    families: list[str]\n",
    "    parameter_size: str\n",
    "    quantization_level: str\n",
    "\n",
    "class OllamaModel(BaseModel):\n",
    "    name: str\n",
    "    model: str\n",
    "    modified_at: datetime\n",
    "    size: int\n",
    "    digest: str\n",
    "    details: OllamaModelDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0509e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adde2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use requests to get the model list\n",
    "def get_model_list() -> list[OllamaModel]:\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/api/tags\")\n",
    "        response.raise_for_status()\n",
    "        ollama_models_json = response.json().get(\"models\", [])\n",
    "        try:\n",
    "            return TypeAdapter(list[OllamaModel]).validate_python(ollama_models_json)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error validating model list: {e}\")\n",
    "            return []\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching model list: {e}\")\n",
    "        pass\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing model list: {e}\")\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        pass\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d87f8015",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_models = get_model_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c058104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: llama3.2:latest (llama3.2:latest)\n",
      "1: llama3.2:3b (llama3.2:3b)\n",
      "2: nomic-embed-text:latest (nomic-embed-text:latest)\n",
      "3: deepseek-coder:6.7b (deepseek-coder:6.7b)\n",
      "4: deepseek-r1:14b (deepseek-r1:14b)\n",
      "5: gemma3:12b (gemma3:12b)\n",
      "6: gemma3:4b (gemma3:4b)\n"
     ]
    }
   ],
   "source": [
    "for index, ollama_model in enumerate(ollama_models):\n",
    "    print(f\"{index}: {ollama_model.name} ({ollama_model.model})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd805c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_prompt_1 = \"\"\"\n",
    "You wear dual hats as an executive assistant and research assistant, you job it to help break down complext tasks into actionable\n",
    "insights or queries.\n",
    "\n",
    "What are the steps you'd need to take or questions you'd need answered in order to resolve the question below.\n",
    "**Ask questions that could be reasonably answered by the entity states of a sophisticated home assistant deployment.**\n",
    "**Ask steps that could be reasonably taken by the devices of a sophisticated home assistant deployment.**\n",
    "\n",
    "Repsond in a numbered list without any additional commentary.\n",
    "\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7f1be60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_prompt_2 = \"\"\"\n",
    "You are a planning agent tasked with designing a simple directed graph of departments that will collaboratively answer a user's question.\n",
    "\n",
    "Each department is a node. Each node:\n",
    "- Has a name ** You don't need to include \"Department\" in the name ** \n",
    "- Has a responsibility or domain\n",
    "- May depend on another departmentâ€™s answer\n",
    "\n",
    "When one department depends on another, define an edge:\n",
    "- From the source node\n",
    "- To the target node\n",
    "** The source node is asking the target node a question. **\n",
    "\n",
    "We'll construct the graph upside down, start with a PR department that compiles the information from\n",
    "the departments and provides a concise answer to the user's direct question/prompt.\n",
    "\n",
    "The PR department will ask the other departments for their input.\n",
    "In turn some or all of those departments may need to ask other departments for their input.\n",
    "The graph should be directed and acyclic.\n",
    "The PR department will be the root node.\n",
    "\n",
    "Output a YAML object with the following template:\n",
    "```yaml\n",
    "graph:\n",
    "    nodes:\n",
    "        - name: <name>\n",
    "          responsibility: <responsibility>\n",
    "        - name: <name>\n",
    "          responsibility: <responsibility>\n",
    "    edges:\n",
    "        - from: <source_node_name>\n",
    "          to: <target_node_name>\n",
    "          question: <question>\n",
    "```\n",
    "\n",
    "Only include relevant departments. Be concise and logical.\n",
    "\n",
    "Do not add explanations or commentary. Only return the YAML graph.\n",
    "\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "61d95988",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = Agent(\n",
    "    model=model,\n",
    "    system_message=manager_prompt_1,\n",
    ")\n",
    "agent_2 = Agent(\n",
    "    model=model,\n",
    "    system_message=manager_prompt_2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a0e0cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Turn on all the christmas lights?\"\n",
    "\n",
    "considerations = agent_1.run_sync(f'{manager_prompt_1}\\n\\nQuestion: {question}')\n",
    "result = agent_2.run_sync(f'{manager_prompt_2}\\n\\nConsiderations: {considerations.output}\\n\\nQuestion: Is today a good day to swim in the pool?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d1ad7044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Turn on all the christmas lights?\n",
      "\n",
      "\n",
      "Considerations:\n",
      "1. Are Christmas lights currently turned off?\n",
      "2. Is the presence of Christmas lights detected by the home assistant's lighting sensor?\n",
      "3. Does the home assistant require power to control the lighting system?\n",
      "4. Can the home assistant send a command to turn on the Christmas lights through its current setup?\n",
      "5. Do Christmas lights have a default \"on\" state, or can they be set to \"auto-on\" upon detection of ambient light conditions?\n",
      "\n",
      "\n",
      "```yaml\n",
      "graph:\n",
      "    nodes:\n",
      "        \n",
      "        - name: Pool Maintenance\n",
      "          responsibility: Monitoring pool water quality and temperature\n",
      "        \n",
      "        - name: Ambiance Lighting\n",
      "          responsibility: Setting lighting ambiance for pool area\n",
      "        \n",
      "        - name: Smart Home System\n",
      "          responsibility: Controlling and managing smart home devices\n",
      "        \n",
      "        - name: Power Distribution\n",
      "          responsibility: Distributing power to various smart home systems\n",
      "        \n",
      "        - name: Christmas Light Installation\n",
      "          responsibility: Installing and maintaining Christmas lights\n",
      "        \n",
      "    edges:\n",
      "    \n",
      "        - from: Pool Maintenance\n",
      "          to: Smart Home System\n",
      "          question: Does the home assistant require power to control the lighting system?\n",
      "        \n",
      "        - from: Ambiance Lighting\n",
      "          to: Power Distribution\n",
      "          question: Does the home assistant send a command to turn on the Christmas lights through its current setup?\n",
      "        \n",
      "        - from: Christmas Light Installation\n",
      "          to: Smart Home System\n",
      "          question: Is the presence of Christmas lights detected by the home assistant's lighting sensor?\n",
      "        \n",
      "        - from: Pool Maintenance\n",
      "          to: Ambiance Lighting\n",
      "          question: Can Christmas lights be set to \"auto-on\" upon detection of ambient light conditions?\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", question)\n",
    "print(\"\\n\")\n",
    "print(\"Considerations:\")\n",
    "print(considerations.output)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b1b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35538f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
